{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from mat6115.hidden import run_and_save_hidden, get_hidden\n",
    "from mat6115.dataset import dataset_factory, TEXT, LABEL, SEED\n",
    "from mat6115.model import RNN\n",
    "from mat6115.train import custom_loss, acc\n",
    "from mat6115.fixed_point import FixedPointFinder\n",
    "from mat6115.analysis import load_model\n",
    "from poutyne.framework import Model\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = dataset_factory(\"imdb\", embedding=\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<poutyne.framework.model.Model at 0x7fe691c2adf0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "SAVE_PATH = Path('gru_1layer')\n",
    "vanilla_model = load_model(SAVE_PATH)\n",
    "trained_model = load_model(SAVE_PATH, restore=True)\n",
    "\n",
    "vanilla_model.to(device)\n",
    "trained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = model.predict((tensor, length_tensor), batch_size=1)\n",
    "    return 1 * LABEL.vocab.stoi['neg'] - 1 / (1 + np.exp(-prediction[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05614579], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(trained_model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47799277], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(vanilla_model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_hidden_states, vanilla_preds, vanilla_ground_truth = get_hidden(\n",
    "    vanilla_model, test_iter, N=5000\n",
    ")\n",
    "trained_hidden_states, trained_preds, trained_ground_truth = get_hidden(\n",
    "    trained_model, test_iter, N=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = vanilla_hidden_states.shape[0]\n",
    "\n",
    "pca_vanilla = [PCA() for _ in range(num_layers)]\n",
    "pca_trained = [PCA() for _ in range(num_layers)]\n",
    "\n",
    "for i in range(num_layers):\n",
    "    pca_vanilla[i].fit(vanilla_hidden_states[i])\n",
    "    pca_trained[i].fit(trained_hidden_states[i])\n",
    "    \n",
    "with open(SAVE_PATH / 'pca_trained.pkl', 'wb') as pca_trained_file:\n",
    "    pickle.dump(pca_trained, pca_trained_file)\n",
    "    \n",
    "with open(SAVE_PATH / 'pca_vanilla.pkl', 'wb') as pca_vanilla_file:\n",
    "    pickle.dump(pca_vanilla, pca_vanilla_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    1, num_layers, \n",
    "    figsize=(6 * num_layers, 5)\n",
    ")\n",
    "if num_layers == 1:\n",
    "    ax = [ax]\n",
    "\n",
    "ax[0].set_ylabel('Explained Variance')\n",
    "for i in range(num_layers):\n",
    "    ax[i].scatter(\n",
    "        np.arange(len(pca_vanilla[i].explained_variance_ratio_)),\n",
    "        pca_vanilla[i].explained_variance_ratio_.cumsum(),\n",
    "        s=2,\n",
    "        c=\"#D9D9D9\"\n",
    "    )\n",
    "    ax[i].scatter(\n",
    "        np.arange(len(pca_trained[i].explained_variance_ratio_)),\n",
    "        pca_trained[i].explained_variance_ratio_.cumsum(),\n",
    "        s=2,\n",
    "        c=\"#282828\"\n",
    "    )\n",
    "    ax[i].set_xlabel('PCA Components')\n",
    "    ax[i].legend(['Untrained', 'Trained'])\n",
    "    ax[i].set_title(f'Layer {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_vanilla_2 = [PCA(n_components=2) for _ in range(num_layers)]\n",
    "pca_trained_2 = [PCA(n_components=2) for _ in range(num_layers)]\n",
    "\n",
    "\n",
    "vanilla = np.empty((num_layers, vanilla_hidden_states.shape[1], 2))\n",
    "trained = np.empty((num_layers, trained_hidden_states.shape[1], 2))\n",
    "for i in range(vanilla_hidden_states.shape[0]):\n",
    "    vanilla[i] = pca_vanilla_2[i].fit_transform(vanilla_hidden_states[i])\n",
    "    trained[i] = pca_trained_2[i].fit_transform(trained_hidden_states[i])\n",
    "    \n",
    "with open(SAVE_PATH / 'pca_trained_2.pkl', 'wb') as pca_trained_file:\n",
    "    pickle.dump(pca_trained_2, pca_trained_file)\n",
    "    \n",
    "with open(SAVE_PATH / 'pca_vanilla_2.pkl', 'wb') as pca_vanilla_file:\n",
    "    pickle.dump(pca_vanilla_2, pca_vanilla_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 2, figsize=(6, 6))\n",
    "pos_idx = vanilla_ground_truth == LABEL.vocab.stoi['pos']\n",
    "neg_idx = vanilla_ground_truth == LABEL.vocab.stoi['neg']\n",
    "ax[0][0].scatter(vanilla_data[pos_idx, 0], vanilla_data[pos_idx, 1], c='g', alpha=0.009, s=2)\n",
    "ax[0][0].set_title('Untrained ground truth - Pos')\n",
    "ax[0][0].set_ylabel(\"PCA component #2\")\n",
    "\n",
    "ax[1][0].scatter(vanilla_data[neg_idx, 0], vanilla_data[neg_idx, 1], c='r', alpha=0.009, s=2)\n",
    "ax[1][0].set_title('Untrained ground truth - Neg')\n",
    "ax[1][0].set_ylabel(\"PCA component #2\")\n",
    "\n",
    "pos_idx = trained_ground_truth == LABEL.vocab.stoi['pos']\n",
    "neg_idx = trained_ground_truth == LABEL.vocab.stoi['neg']\n",
    "ax[0][1].scatter(trained_data[pos_idx, 0], trained_data[pos_idx, 1], c='g', alpha=0.009, s=2)\n",
    "ax[0][1].set_title('Trained ground truth - Pos')\n",
    "\n",
    "ax[1][1].scatter(trained_data[neg_idx, 0], trained_data[neg_idx, 1], c='r', alpha=0.009, s=2)\n",
    "ax[1][1].set_title('Trained ground truth - Neg')\n",
    "\n",
    "pos_idx = vanilla_preds == LABEL.vocab.stoi['pos']\n",
    "neg_idx = vanilla_preds == LABEL.vocab.stoi['neg']\n",
    "ax[2][0].scatter(vanilla_data[pos_idx, 0], vanilla_data[pos_idx, 1], c='g', alpha=0.009, s=2)\n",
    "ax[2][0].set_title('Untrained Preds - Pos')\n",
    "ax[2][0].set_ylabel(\"PCA component #2\")\n",
    "\n",
    "ax[3][0].scatter(vanilla_data[neg_idx, 0], vanilla_data[neg_idx, 1], c='r', alpha=0.009, s=2)\n",
    "ax[3][0].set_title('Untrained Preds - Neg')\n",
    "ax[3][0].set_xlabel(\"PCA component #1\")\n",
    "ax[3][0].set_ylabel(\"PCA component #2\")\n",
    "\n",
    "pos_idx = trained_preds == LABEL.vocab.stoi['pos']\n",
    "neg_idx = trained_preds == LABEL.vocab.stoi['neg']\n",
    "ax[2][1].scatter(trained_data[pos_idx, 0], trained_data[pos_idx, 1], c='g', alpha=0.009, s=2)\n",
    "ax[2][1].set_title('Trained Preds - Pos')\n",
    "\n",
    "ax[3][1].scatter(trained_data[neg_idx, 0], trained_data[neg_idx, 1], c='r', alpha=0.009, s=2)\n",
    "ax[3][1].set_title('Trained Preds - Neg')\n",
    "ax[3][1].set_xlabel(\"PCA component #1\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca_trained_3 = PCA(n_components=3)\n",
    "\n",
    "trained_data_3 = pca_trained_3.fit_transform(trained_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(6, 6))\n",
    "random_idx = np.arange(len(trained_data_3))\n",
    "np.random.shuffle(random_idx)\n",
    "random_idx = random_idx[:1000]\n",
    "\n",
    "pos_idx = vanilla_ground_truth[random_idx] == LABEL.vocab.stoi['pos']\n",
    "neg_idx = vanilla_ground_truth[random_idx] == LABEL.vocab.stoi['neg']\n",
    "print(ax.shape)\n",
    "\n",
    "for i, j in product([0,1,2], [0,1,2]):\n",
    "    ax[i][j].scatter(\n",
    "        trained_data_3[random_idx][pos_idx, i], \n",
    "        trained_data_3[random_idx][pos_idx, j], \n",
    "        c='g', alpha=0.2, s=2\n",
    "    )\n",
    "    ax[i][j].scatter(\n",
    "        trained_data_3[random_idx][neg_idx, i], \n",
    "        trained_data_3[random_idx][neg_idx, j], \n",
    "        c='r', alpha=0.2, s=2\n",
    "    )\n",
    "    if j == 0:\n",
    "        ax[i][j].set_ylabel(f\"Component #{i+1}\")\n",
    "    if i == 2:\n",
    "        ax[i][j].set_xlabel(f\"Component #{j+1}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca_trained_3 = PCA(n_components=3)\n",
    "\n",
    "trained_data_3 = pca_trained_3.fit_transform(trained_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "random_idx = np.arange(len(trained_data_3))\n",
    "np.random.shuffle(random_idx)\n",
    "random_idx = random_idx[:3000]\n",
    "\n",
    "pos_idx = trained_ground_truth[random_idx] == LABEL.vocab.stoi['pos']\n",
    "neg_idx = trained_ground_truth[random_idx] == LABEL.vocab.stoi['neg']\n",
    "\n",
    "ax.scatter3D(\n",
    "    trained_data_3[random_idx][pos_idx,0],\n",
    "    trained_data_3[random_idx][pos_idx,1],\n",
    "    trained_data_3[random_idx][pos_idx,2],\n",
    "    c='g', s=2, alpha=0.3\n",
    ")\n",
    "\n",
    "ax.scatter3D(\n",
    "    trained_data_3[random_idx][neg_idx,0],\n",
    "    trained_data_3[random_idx][neg_idx,1],\n",
    "    trained_data_3[random_idx][neg_idx,2],\n",
    "    c='r', s=2, alpha=0.3\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Component #1')\n",
    "ax.set_ylabel('Component #2')\n",
    "ax.set_zlabel('Component #3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_data_3[random_idx][pos_idx,2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_point_finder = FixedPointFinder(\n",
    "    rnn_cell=trained_model.network.rnn[0], \n",
    "    lr=0.01, \n",
    "    n_iter=200, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_hidden_states = torch.tensor(trained_hidden_states).to(device)\n",
    "constant_input = torch.zeros((trained_hidden_states.shape[1], 1, kwargs['embedding_dim'])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 498968, 256])\n",
      "torch.Size([498968, 1, 100])\n"
     ]
    }
   ],
   "source": [
    "print(trained_hidden_states.shape)\n",
    "print(constant_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 0.03s/step Step 4: loss: 4388.904785"
     ]
    }
   ],
   "source": [
    "point, is_fixed_point = fixed_point_finder.run(\n",
    "    trained_hidden_states[:, :1000], \n",
    "    constant_input[:1000], \n",
    "    batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = [TEXT.vocab.stoi[TEXT.pad_token]]\n",
    "length = [1]\n",
    "tensor = torch.LongTensor(indexed).to(device)\n",
    "tensor = tensor.unsqueeze(0)\n",
    "length_tensor = torch.LongTensor(length)\n",
    "    \n",
    "for layer in trained_hidden_states:\n",
    "    for state in layer:\n",
    "        state = torch.LongTensor(state).unsqueeze(0).unsqueeze(0)\n",
    "        _, _, _, hidden_state = trained_model.network(tensor, length_tensor, state)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
