{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Updated Sentiment Analysis\n",
    "\n",
    "In the previous notebook, we got the fundamentals down for sentiment analysis. In this notebook, we'll actually get decent results.\n",
    "\n",
    "We will use:\n",
    "- packed padded sequences\n",
    "- pre-trained word embeddings\n",
    "- different RNN architecture\n",
    "- bidirectional RNN\n",
    "- multi-layer RNN\n",
    "- regularization\n",
    "- a different optimizer\n",
    "\n",
    "This will allow us to achieve ~84% test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "As before, we'll set the seed, define the `Fields` and get the train/valid/test splits.\n",
    "\n",
    "We'll be using *packed padded sequences*, which will make our RNN only process the non-padded elements of our sequence, and for any padded element the `output` will be a zero tensor. To use packed padded sequences, we have to tell the RNN how long the actual sequences are. We do this by setting `include_lengths = True` for our `TEXT` field. This will cause `batch.text` to now be a tuple with the first element being our sentence (a numericalized tensor that has been padded) and the second element being the actual lengths of our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from poutyne.framework import Model\n",
    "from poutyne.framework.callbacks import ModelCheckpoint\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "from mat6115.model import RNN\n",
    "from mat6115.dataset import dataset_factory, TEXT, LABEL, SEED\n",
    "from mat6115.train import acc, custom_loss\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dataset factory to get IMDb dataset, pre split in train-valid-test iterators. Many pretrained word vectors are available.\n",
    "\n",
    "- charngram.100d\n",
    "- fasttext.en.300d\n",
    "- fasttext.simple.300d\n",
    "- glove.42B.300d\n",
    "- glove.840B.300d\n",
    "- glove.twitter.27B.25d\n",
    "- glove.twitter.27B.50d\n",
    "- glove.twitter.27B.100d\n",
    "- glove.twitter.27B.200d\n",
    "- glove.6B.50d\n",
    "- glove.6B.100d\n",
    "- glove.6B.200d\n",
    "- glove.6B.300d\n",
    "\n",
    "> Note: these vectors are over 800MB, so watch out if you have a limited internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = dataset_factory(\"imdb\", embedding=\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexis/miniconda3/envs/mat6115/lib/python3.8/site-packages/torch/nn/modules/rnn.py:47: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "kwargs = dict(\n",
    "    vocab_size = len(TEXT.vocab),\n",
    "    embedding_dim = 100,\n",
    "    hidden_dim = 256,\n",
    "    output_dim = 1,\n",
    "    n_layers = 1,\n",
    "    dropout = 0.5,\n",
    "    pad_idx = TEXT.vocab.stoi[TEXT.pad_token],\n",
    "    null_idx = TEXT.vocab.stoi[TEXT.null_token],\n",
    "    rnn_type = 'gru'\n",
    ")\n",
    "model = RNN(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,775,501 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final addition is copying the pre-trained word embeddings we loaded earlier into the `embedding` layer of our model.\n",
    "\n",
    "We retrieve the embeddings from the field's vocab, and check they're the correct size, _**[vocab size, embedding dim]**_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25003, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then replace the initial weights of the `embedding` layer with the pre-trained embeddings.\n",
    "\n",
    "**Note**: this should always be done on the `weight.data` and not the `weight`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
       "        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n",
       "        [-0.4611, -0.0639, -1.3667,  ...,  1.6309, -0.0847,  1.0844],\n",
       "        ...,\n",
       "        [ 0.2766, -0.2792, -0.1118,  ..., -0.8459, -0.5144, -0.1363],\n",
       "        [ 0.2785, -0.5053,  0.7145,  ..., -0.2789, -0.4981,  0.5644],\n",
       "        [ 0.6178, -0.2024,  0.0910,  ...,  0.3046, -0.6263,  0.5276]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our `<unk>`, `<pad>` and `<null>` token aren't in the pre-trained vocabulary they have been initialized using `unk_init` (an $\\mathcal{N}(0,1)$ distribution) when building our vocab. It is preferable to initialize all three to all zeros to explicitly tell our model that, initially, they are irrelevant for determining sentiment. \n",
    "\n",
    "We do this by manually setting their row in the embedding weights matrix to zeros. We get their row by finding the index of the tokens. The `<null>` is rather special, it should never be trained. It is used to study fixed points or quasi-fixed points ($h \\approx F(h, 0)$)\n",
    "\n",
    "> Note: like initializing the embeddings, this should be done on the `weight.data` and not the `weight`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2766, -0.2792, -0.1118,  ..., -0.8459, -0.5144, -0.1363],\n",
      "        [ 0.2785, -0.5053,  0.7145,  ..., -0.2789, -0.4981,  0.5644],\n",
      "        [ 0.6178, -0.2024,  0.0910,  ...,  0.3046, -0.6263,  0.5276]])\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "NULL_IDX = TEXT.vocab.stoi[TEXT.null_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(kwargs['embedding_dim'])\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(kwargs['embedding_dim'])\n",
    "model.embedding.weight.data[NULL_IDX] = torch.zeros(kwargs['embedding_dim'])\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the first two rows of the embedding weights matrix have been set to zeros. As we passed the index of the pad token to the `padding_idx` of the embedding layer it will remain zeros throughout training, however the `<unk>` token embedding will be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to training the model.\n",
    "\n",
    "The only change we'll make here is changing the optimizer from `SGD` to `Adam`. SGD updates all parameters with the same learning rate and choosing this learning rate can be tricky. `Adam` adapts the learning rate for each parameter, giving parameters that are updated more frequently lower learning rates and parameters that are updated infrequently higher learning rates. More information about `Adam` (and other optimizers) can be found [here](http://ruder.io/optimizing-gradient-descent/index.html).\n",
    "\n",
    "To change `SGD` to `Adam`, we simply change `optim.SGD` to `optim.Adam`, also note how we do not have to provide an initial learning rate for Adam as PyTorch specifies a sensibile default initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for training our model. \n",
    "\n",
    "As we have set `include_lengths = True`, our `batch.text` is now a tuple with the first element being the numericalized tensor and the second element being the actual lengths of each sequence. We separate these into their own variables, `text` and `text_lengths`, before passing them to the model.\n",
    "\n",
    "**Note**: as we are now using dropout, we must remember to use `model.train()` to ensure the dropout is \"turned on\" while training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 7.54s Step 274/274: loss: 0.636729, acc: 0.627600, val_loss: 0.701215, val_acc: 0.660800\n",
      "Epoch 2/10 7.48s Step 274/274: loss: 0.421709, acc: 0.811314, val_loss: 0.322023, val_acc: 0.866000\n",
      "Epoch 3/10 7.49s Step 274/274: loss: 0.271525, acc: 0.888400, val_loss: 0.252827, val_acc: 0.899200\n",
      "Epoch 4/10 7.46s Step 274/274: loss: 0.212121, acc: 0.917429, val_loss: 0.241873, val_acc: 0.906533\n",
      "Epoch 5/10 7.46s Step 274/274: loss: 0.174293, acc: 0.933543, val_loss: 0.232285, val_acc: 0.909733\n",
      "Epoch 6/10 7.49s Step 274/274: loss: 0.141388, acc: 0.948343, val_loss: 0.256034, val_acc: 0.907333\n",
      "Epoch 7/10 7.47s Step 274/274: loss: 0.115610, acc: 0.959257, val_loss: 0.257615, val_acc: 0.910400\n",
      "Epoch 8/10 7.48s Step 274/274: loss: 0.096256, acc: 0.965829, val_loss: 0.271390, val_acc: 0.897067\n",
      "Epoch 9/10 7.45s Step 274/274: loss: 0.081667, acc: 0.971314, val_loss: 0.315175, val_acc: 0.905600\n",
      "Epoch 10/10 7.48s Step 274/274: loss: 0.071324, acc: 0.974571, val_loss: 0.314905, val_acc: 0.909200\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "SAVE_PATH = Path(f'{kwargs[\"rnn_type\"]}_{kwargs[\"n_layers\"]}layer')\n",
    "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "with open(SAVE_PATH / 'kwargs.json', 'w') as kwargs_file:\n",
    "    json.dump(kwargs, kwargs_file)\n",
    "    \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "poutyne_model = Model(\n",
    "    network=model, \n",
    "    optimizer=optimizer, \n",
    "    loss_function=custom_loss, \n",
    "    batch_metrics=[acc]\n",
    ")\n",
    "poutyne_model.to(device)\n",
    "\n",
    "history = poutyne_model.fit_generator(\n",
    "    train_generator=train_iter, \n",
    "    valid_generator=valid_iter, \n",
    "    epochs=N_EPOCHS,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            filename=str(SAVE_PATH / \"model.pkl\"),\n",
    "            save_best_only=True,\n",
    "            restore_best=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2569, Test Binary Accuracy: 0.8988\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, y_pred, y_true = poutyne_model.evaluate_generator(\n",
    "    generator=test_iter, \n",
    "    return_pred=True, \n",
    "    return_ground_truth=True\n",
    ")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Binary Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input\n",
    "\n",
    "We can now use our model to predict the sentiment of any sentence we give it. As it has been trained on movie reviews, the sentences provided should also be movie reviews.\n",
    "\n",
    "When using a model for inference it should always be in evaluation mode. If this tutorial is followed step-by-step then it should already be in evaluation mode (from doing `evaluate` on the test set), however we explicitly set it to avoid any risk.\n",
    "\n",
    "Our `predict_sentiment` function does a few things:\n",
    "- sets the model to evaluation mode\n",
    "- tokenizes the sentence, i.e. splits it from a raw string into a list of tokens\n",
    "- indexes the tokens by converting them into their integer representation from our vocabulary\n",
    "- gets the length of our sequence\n",
    "- converts the indexes, which are a Python list into a PyTorch tensor\n",
    "- add a batch dimension by `unsqueeze`ing \n",
    "- converts the length into a tensor\n",
    "- squashes the output prediction from a real number between 0 and 1 with the `sigmoid` function\n",
    "- converts the tensor holding a single value into an integer with the `item()` method\n",
    "\n",
    "We are expecting reviews with a negative sentiment to return a value close to 0 and positive reviews to return a value close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = model.predict((tensor, length_tensor), batch_size=1)\n",
    "    return 1 * LABEL.vocab.stoi['neg'] - 1 / (1 + np.exp(-prediction[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example negative review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09787267], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(poutyne_model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example positive review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7957635], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(poutyne_model, \"This film is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We've now built a decent sentiment analysis model for movie reviews! In the next notebook we'll implement a model that gets comparable accuracy with far fewer parameters and trains much, much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
